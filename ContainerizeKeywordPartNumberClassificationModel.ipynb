{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a94159d",
   "metadata": {},
   "source": [
    "### Create directory and copy model and vectorizer files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1baa7c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a directory to contain the model\n",
    "#!mkdir .\\keyword-partnumber-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36556344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1 file(s) copied.\n"
     ]
    }
   ],
   "source": [
    "# copy the model to the new directory\n",
    "#!copy .\\keyword_partnumber_classmodel.pickle .\\keyword-partnumber-classification\n",
    "#!copy .\\generate-term-data.pickle .\\keyword-partnumber-classification\n",
    "#!copy .\\generate_keyword_data.py .\\keyword-partnumber-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6a00b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.svm import LinearSVC\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class keywordpartnumberclassifier():\n",
    "    # one time initialization of model and dependent objects\n",
    "    def __init__(self):\n",
    "        with open(\"generate-term-data.pickle\", \"rb\") as f:\n",
    "            self.vectorizer = pickle.load(f)\n",
    "        with open(\"keyword_partnumber_classmodel.pickle\", \"rb\") as f:\n",
    "            self.model = pickle.load(f)\n",
    "        # dummy prediction\n",
    "        print(self.predict(X = np.array([\"800v\"]), names = [\"term\"]))\n",
    "        \n",
    "    def predict(self, X, names):\n",
    "        # get search term from X\n",
    "        model_input = dict(zip(names, X))\n",
    "        search_term = model_input.get(\"term\")\n",
    "        # get features from search term\n",
    "        features = self.vectorizer(search_term)\n",
    "        features.pop(0) # remove term from feature column data\n",
    "        features.pop() # remove empty space for isPartNumber\n",
    "        features = np.array(features, dtype=int).reshape(1, -1) # reshape to 2D array\n",
    "        prediction = self.model.predict(features)\n",
    "        return [int(prediction[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0629fced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "kpc = keywordpartnumberclassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437409ee",
   "metadata": {},
   "source": [
    "### Create a docker image\n",
    "1. Create requirements.txt file\n",
    "2. Create Dockerfile\n",
    "3. Create keywordpartnumberclassifier class with prediction logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e78b932b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .\\keyword-partnumber-classification\\requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile .\\keyword-partnumber-classification\\requirements.txt\n",
    "seldon-core\n",
    "scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e7c0a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .\\keyword-partnumber-classification\\keywordpartnumberclassifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile .\\keyword-partnumber-classification\\keywordpartnumberclassifier.py\n",
    "import pickle\n",
    "from sklearn.svm import LinearSVC\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class keywordpartnumberclassifier():\n",
    "    # one time initialization of model and dependent objects\n",
    "    def __init__(self):\n",
    "        with open(\"generate-term-data.pickle\", \"rb\") as f:\n",
    "            self.vectorizer = pickle.load(f)\n",
    "        with open(\"keyword_partnumber_classmodel.pickle\", \"rb\") as f:\n",
    "            self.model = pickle.load(f)\n",
    "        \n",
    "    def predict(self, X, names):\n",
    "        # get search term from X\n",
    "        model_input = dict(zip(names, X))\n",
    "        search_term = model_input.get(\"term\")\n",
    "        # get features from search term\n",
    "        features = self.vectorizer(search_term)\n",
    "        features.pop(0) # remove term from feature column data\n",
    "        features.pop() # remove empty space for isPartNumber\n",
    "        features = np.array(features, dtype=int).reshape(1, -1) # reshape to 2D array\n",
    "        prediction = self.model.predict(features)\n",
    "        return [int(prediction[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bc45471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing .\\keyword-partnumber-classification\\Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile .\\keyword-partnumber-classification\\Dockerfile\n",
    "FROM python:3.7-slim\n",
    "WORKDIR /app\n",
    "\n",
    "# Install python packages\n",
    "COPY requirements.txt requirements.txt\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "# Copy source code\n",
    "COPY . .\n",
    "\n",
    "# Port for GRPC\n",
    "EXPOSE 5000\n",
    "# Port for REST\n",
    "EXPOSE 9000\n",
    "\n",
    "# Define environment variables\n",
    "ENV MODEL_NAME keywordpartnumberclassifier\n",
    "ENV SERVICE_TYPE MODEL\n",
    "\n",
    "# Changing folder to default user\n",
    "RUN chown -R 8888 /app\n",
    "\n",
    "CMD exec seldon-core-microservice $MODEL_NAME --service-type $SERVICE_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f07305",
   "metadata": {},
   "source": [
    "### Build docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "727e1d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#0 building with \"default\" instance using docker driver\n",
      "\n",
      "#1 [internal] load build definition from Dockerfile\n",
      "#1 transferring dockerfile: 516B done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [internal] load .dockerignore\n",
      "#2 transferring context: 2B done\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [internal] load metadata for docker.io/library/python:3.7-slim\n",
      "#3 ...\n",
      "\n",
      "#4 [auth] library/python:pull token for registry-1.docker.io\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#3 [internal] load metadata for docker.io/library/python:3.7-slim\n",
      "#3 DONE 0.9s\n",
      "\n",
      "#5 [1/6] FROM docker.io/library/python:3.7-slim@sha256:b53f496ca43e5af6994f8e316cf03af31050bf7944e0e4a308ad86c001cf028b\n",
      "#5 DONE 0.0s\n",
      "\n",
      "#6 [internal] load build context\n",
      "#6 transferring context: 1.27kB done\n",
      "#6 DONE 0.0s\n",
      "\n",
      "#7 [2/6] WORKDIR /app\n",
      "#7 CACHED\n",
      "\n",
      "#8 [3/6] COPY requirements.txt requirements.txt\n",
      "#8 CACHED\n",
      "\n",
      "#9 [4/6] RUN pip install -r requirements.txt\n",
      "#9 CACHED\n",
      "\n",
      "#10 [5/6] COPY . .\n",
      "#10 DONE 0.1s\n",
      "\n",
      "#11 [6/6] RUN chown -R 8888 /app\n",
      "#11 DONE 0.4s\n",
      "\n",
      "#12 exporting to image\n",
      "#12 exporting layers\n",
      "#12 exporting layers 0.1s done\n",
      "#12 writing image sha256:08edc3309dc5d21ab86c312fb6ff225b22fd527e26b262398e8991c8687aace3 0.0s done\n",
      "#12 naming to docker.io/library/keywordpartnumberclassifier:2.1 done\n",
      "#12 DONE 0.2s\n",
      "\n",
      "What's Next?\n",
      "  View a summary of image vulnerabilities and recommendations â†’ docker scout quickview\n"
     ]
    }
   ],
   "source": [
    "!docker build .\\keyword-partnumber-classification\\ -t keywordpartnumberclassifier:2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95cb1f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keywordpartnumberclassifier                               2.1       08edc3309dc5   5 seconds ago   573MB\n",
      "keywordpartnumberclassifier                               2.0       874c953e1ac4   5 minutes ago   573MB\n",
      "deneemcclain/keywordpartnumberclassifier                  1.8       671b2f74676d   3 months ago    573MB\n",
      "keywordpartnumberclassifier                               1.8       671b2f74676d   3 months ago    573MB\n",
      "keywordpartnumberclassifier                               1.7       9d7c6d17e34f   3 months ago    573MB\n",
      "keywordpartnumberclassifier                               1.6       28c96a6c9e67   3 months ago    573MB\n",
      "deneemcclain/keywordpartnumberclassifier                  1.5       d12f6d1c9df4   3 months ago    573MB\n",
      "keywordpartnumberclassifier                               1.5       d12f6d1c9df4   3 months ago    573MB\n",
      "deneemcclain/keywordpartnumberclassifier                  1.4       e9e8e8d74d4c   3 months ago    573MB\n",
      "keywordpartnumberclassifier                               1.4       e9e8e8d74d4c   3 months ago    573MB\n",
      "mousercontainers.azurecr.io/keywordpartnumberclassifier   1.4       e9e8e8d74d4c   3 months ago    573MB\n"
     ]
    }
   ],
   "source": [
    "!docker images | findstr keywordpartnumberclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bde1c340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\":{\"names\":[],\"ndarray\":[0]},\"meta\":{}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100    95  100    46  100    49  12806  13641 --:--:-- --:--:-- --:--:-- 31666\n"
     ]
    }
   ],
   "source": [
    "# test the model returns as expected by running the image and sending a request to it\n",
    "#!curl.exe -g http://localhost:2732/predict -H \"Content-Type: application/json\" --data @test-prediction.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1145e5e9",
   "metadata": {},
   "source": [
    "### Tag the docker image and push to a repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b13e170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag the desired image with a new name that you want to display in the desired repository\n",
    "#!docker tag <image_name> <new_image_name_with_tag>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59b238ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# login to docker to push to the desired repository\n",
    "#!docker login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63ff23bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# push the desired image to the authenticated repository\n",
    "#!docker push <image_name_and_tag>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
